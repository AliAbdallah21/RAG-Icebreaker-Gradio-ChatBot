{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPotCv6FBoQfpLTFeI8MHte",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliAbdallah21/RAG-Icebreaker-Gradio-ChatBot/blob/main/AI_Icebreaker_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nsrnsIzjo0s3"
      },
      "outputs": [],
      "source": [
        "# @title All the pip installs\n",
        "%%capture\n",
        "!pip install chromadb\n",
        "!pip install llama_index\n",
        "!pip install llama-index-core.\n",
        "!pip install llama-index-vector-stores-chroma\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install sentence-transformers\n",
        "!pip install PyMuPDF\n",
        "!pip install gradio\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install requests\n",
        "print(\"LlamaIndex and its integrations installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title All the imports\n",
        "import gradio as gr\n",
        "from llama_index.core import VectorStoreIndex, StorageContext, SimpleDirectoryReader,Document\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import get_response_synthesizer\n",
        "from llama_index.core.settings import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.readers.file import PyMuPDFReader\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core.chat_engine import CondenseQuestionChatEngine # Import CondenseQuestionChatEngine\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import chromadb\n",
        "import requests\n",
        "import json\n",
        "import time"
      ],
      "metadata": {
        "id": "BRMTn3q6pArV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title OpenAI API Key Setup\n",
        "try:\n",
        "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"OpenAI API key loaded from Colab secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading OpenAI API key from Colab secrets: {e}\")\n",
        "    print(\"Please ensure your 'OPENAI_API_KEY' secret is set in Colab and enabled for this notebook.\")\n",
        "    raise SystemExit(\"API Key not found. Exiting.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI126-Hkylf6",
        "outputId": "7778e40f-94da-427e-a3fb-caebf7b7be6f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded from Colab secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Data Acquisition using the Bright Data API\n",
        "def fetch_linkedin_profile_brightdata(linkedin_url: str, brightdata_api_key: str, use_mock_data: bool = False) -> dict:\n",
        "    if use_mock_data:\n",
        "        print(\"Using mock data for LinkedIn profile.\")\n",
        "        mock_data = {\n",
        "            \"id\": \"mock-user-123\",\n",
        "            \"name\": \"Mock User Name\",\n",
        "            \"city\": \"Mock City, Mock Country\",\n",
        "            \"country_code\": \"XX\",\n",
        "            \"position\": \"Lead AI Engineer at MockCorp | Generative AI Specialist\",\n",
        "            \"current_company\": {\n",
        "                \"name\": \"MockCorp\",\n",
        "                \"company_id\": \"mockcorp\",\n",
        "                \"title\": \"Lead AI Engineer\",\n",
        "                \"location\": None\n",
        "            },\n",
        "            \"experience\": [\n",
        "                {\n",
        "                    \"title\": \"Lead AI Engineer\",\n",
        "                    \"description\": \"Developed scalable AI solutions and drove innovation in the tech industry. Specialized in deep learning and natural language processing.\",\n",
        "                    \"start_date\": \"Jan 2022\",\n",
        "                    \"end_date\": \"Present\",\n",
        "                    \"company\": \"MockCorp\",\n",
        "                    \"company_id\": \"mockcorp\",\n",
        "                    \"url\": \"https://www.linkedin.com/company/mockcorp\"\n",
        "                },\n",
        "                {\n",
        "                    \"title\": \"Senior Data Scientist\",\n",
        "                    \"description\": \"Built predictive models and analyzed large datasets to derive business insights. Focused on machine learning algorithms and data visualization.\",\n",
        "                    \"start_date\": \"Sep 2019\",\n",
        "                    \"end_date\": \"Dec 2021\",\n",
        "                    \"company\": \"InnovateAI\",\n",
        "                    \"company_id\": \"innovateai\",\n",
        "                    \"url\": \"https://www.linkedin.com/company/innovateai\"\n",
        "                }\n",
        "            ],\n",
        "            \"url\": linkedin_url,\n",
        "            \"educations_details\": \"Mock University\",\n",
        "            \"education\": [\n",
        "                {\n",
        "                    \"title\": \"Mock University\",\n",
        "                    \"degree\": \"Master's Degree\",\n",
        "                    \"field\": \"Artificial Intelligence\",\n",
        "                    \"start_year\": \"2017\",\n",
        "                    \"end_year\": \"2019\"\n",
        "                }\n",
        "            ],\n",
        "            \"avatar\": \"https://placehold.co/200x200/cccccc/ffffff?text=Mock\",\n",
        "            \"followers\": 100,\n",
        "            \"connections\": 150,\n",
        "            \"projects\": [\n",
        "                {\n",
        "                    \"title\": \"AI-Powered Recommendation System\",\n",
        "                    \"start_date\": \"Mar 2023\",\n",
        "                    \"end_date\": \"Aug 2023\",\n",
        "                    \"description\": \"Developed a novel recommendation engine using collaborative filtering and deep learning techniques, resulting in a 15% increase in user engagement.\"\n",
        "                }\n",
        "            ],\n",
        "            \"location\": \"Mock City\",\n",
        "            \"input_url\": linkedin_url,\n",
        "            \"linkedin_id\": \"mock-user-123\",\n",
        "            \"activity\": [\n",
        "                {\n",
        "                    \"interaction\": \"Liked by Mock User\",\n",
        "                    \"link\": \"https://www.linkedin.com/feed/update/mock-post-1\",\n",
        "                    \"title\": \"Exciting advancements in Generative AI!\",\n",
        "                    \"img\": None,\n",
        "                    \"id\": \"mock-activity-1\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "        return mock_data\n",
        "\n",
        "    # --- Trigger API ---\n",
        "    brightdata_trigger_api_url = \"https://api.brightdata.com/datasets/v3/trigger\"\n",
        "    dataset_id = \"gd_l1viktl72bvl7bjuj0\" # Your specific dataset ID\n",
        "    trigger_query_params = {\n",
        "        \"dataset_id\": dataset_id,\n",
        "        \"include_errors\": \"true\"\n",
        "    }\n",
        "    auth_headers = {\n",
        "        'Authorization': f'Bearer {brightdata_api_key}',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "    trigger_payload = [\n",
        "        {\"url\": linkedin_url}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        print(f\"Fetching profile for: {linkedin_url}\")\n",
        "        response = requests.post(brightdata_trigger_api_url, headers=auth_headers, json=trigger_payload, params=trigger_query_params)\n",
        "        response.raise_for_status()\n",
        "        trigger_response = response.json()\n",
        "\n",
        "        snapshot_id = trigger_response.get('snapshot_id')\n",
        "        if not snapshot_id:\n",
        "            print(\"ERROR: No snapshot_id returned from Bright Data trigger API.\")\n",
        "            return {}\n",
        "\n",
        "        print(f\"Collection triggered. Snapshot ID: {snapshot_id}. Polling for status...\")\n",
        "\n",
        "        # --- Polling for Snapshot Status ---\n",
        "        brightdata_snapshots_list_url = \"https://api.brightdata.com/datasets/v3/snapshots\"\n",
        "        snapshots_list_params = {\n",
        "            \"dataset_id\": dataset_id,\n",
        "            \"status\": \"ready\"\n",
        "        }\n",
        "\n",
        "        max_polling_attempts = 60 # 10 minutes total\n",
        "        polling_interval_seconds = 10\n",
        "        found_ready_snapshot = False\n",
        "\n",
        "        for i in range(max_polling_attempts):\n",
        "            time.sleep(polling_interval_seconds)\n",
        "\n",
        "            list_response = requests.get(brightdata_snapshots_list_url, headers=auth_headers, params=snapshots_list_params)\n",
        "            list_response.raise_for_status()\n",
        "            snapshots_data = list_response.json()\n",
        "\n",
        "            for snapshot in snapshots_data:\n",
        "                if snapshot.get('id') == snapshot_id and snapshot.get('status') == 'ready':\n",
        "                    found_ready_snapshot = True\n",
        "                    break\n",
        "\n",
        "            if found_ready_snapshot:\n",
        "                print(f\"✅ Snapshot {snapshot_id} is ready!\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Polling... ({i+1}/{max_polling_attempts})\")\n",
        "\n",
        "        if not found_ready_snapshot:\n",
        "            print(f\"❌ Snapshot {snapshot_id} did not become ready after {max_polling_attempts} attempts.\")\n",
        "            return {}\n",
        "\n",
        "        # --- Retrieve Data Directly from Snapshot Metadata Endpoint ---\n",
        "        brightdata_snapshot_data_url = f\"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}\"\n",
        "        print(f\"Retrieving data from Bright Data...\")\n",
        "\n",
        "        data_response = requests.get(brightdata_snapshot_data_url, headers=auth_headers)\n",
        "        data_response.raise_for_status()\n",
        "\n",
        "        retrieved_data = data_response.json()\n",
        "\n",
        "        if isinstance(retrieved_data, dict) and retrieved_data.get('name') and retrieved_data.get('linkedin_id'):\n",
        "            print(\"✅ Data retrieved successfully!\")\n",
        "            return retrieved_data\n",
        "        else:\n",
        "            print(\"❌ Retrieved data is empty or not in expected profile format (dict with 'name'/'linkedin_id').\")\n",
        "            return {}\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Request failed: {e}\")\n",
        "        if 'response' in locals() and response is not None:\n",
        "            print(f\"Bright Data Error Response Status Code: {response.status_code}\")\n",
        "            print(f\"Bright Data Error Response Text: {response.text}\")\n",
        "        return {}\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"❌ Failed to decode JSON: {e}\")\n",
        "        if 'response' in locals() and response is not None:\n",
        "            print(f\"Raw response that failed JSON decode: {response.text}\")\n",
        "        return {}\n"
      ],
      "metadata": {
        "id": "Z1A7bP3np4M7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test Data Acquisition\n",
        "# IMPORTANT: Replace with a real, public LinkedIn profile URL for testing.\n",
        "# For GitHub, leave this as a placeholder or a non-personal example.\n",
        "test_linkedin_url = \"https://www.linkedin.com/in/example-user-profile/\" # Changed to a generic placeholder URL\n",
        "\n",
        "# Get your Bright Data API Key from Colab Secrets\n",
        "brightdata_key_from_secrets = userdata.get('BRIGHTDATA_API_KEY')\n",
        "\n",
        "# --- Choose your testing mode ---\n",
        "# Set to True to force mock data. Set to False to try real data first.\n",
        "USE_MOCK_DATA_FOR_TEST = True # Set to True for easier testing without Bright Data credits\n",
        "\n",
        "profile_data = {} # Initialize profile_data to an empty dict\n",
        "\n",
        "if not USE_MOCK_DATA_FOR_TEST and brightdata_key_from_secrets:\n",
        "    print(\"Attempting to fetch real LinkedIn profile data...\")\n",
        "    profile_data = fetch_linkedin_profile_brightdata(\n",
        "        linkedin_url=test_linkedin_url,\n",
        "        brightdata_api_key=brightdata_key_from_secrets,\n",
        "        use_mock_data=False\n",
        "    )\n",
        "    if profile_data:\n",
        "        print(\"Successfully fetched real LinkedIn profile data!\")\n",
        "    else:\n",
        "        print(\"Failed to fetch real LinkedIn profile data. Falling back to mock data.\")\n",
        "        profile_data = fetch_linkedin_profile_brightdata(\n",
        "            linkedin_url=test_linkedin_url,\n",
        "            brightdata_api_key=brightdata_key_from_secrets, # Key not used for mock, but function expects it\n",
        "            use_mock_data=True # Use mock data for fallback\n",
        "        )\n",
        "else:\n",
        "    print(\"Bright Data API key not found in Colab Secrets or USE_MOCK_DATA_FOR_TEST is True. Using mock data.\")\n",
        "    profile_data = fetch_linkedin_profile_brightdata(\n",
        "        linkedin_url=test_linkedin_url,\n",
        "        brightdata_api_key=brightdata_key_from_secrets, # Key not used for mock, but function expects it\n",
        "        use_mock_data=True # Use mock data if key not found or forced\n",
        "    )\n",
        "\n",
        "if profile_data:\n",
        "    print(\"\\nProfile data is now available (either real or mock).\")\n",
        "    print(f\"Name: {profile_data.get('name', 'N/A')}\")\n",
        "    print(f\"Headline: {profile_data.get('position', 'N/A')}\")\n",
        "    print(f\"Number of experiences: {len(profile_data.get('experience', []))}\")\n",
        "    print(\"\\nFull Profile Data Structure (first 500 chars):\")\n",
        "    print(json.dumps(profile_data, indent=2)[:500] + \"...\")\n",
        "else:\n",
        "    print(\"CRITICAL: Profile data is still empty. Cannot proceed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmxFQfMeysxL",
        "outputId": "c7630068-fa1b-47cf-a676-f0436e4ad68a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bright Data API key not found in Colab Secrets or USE_MOCK_DATA_FOR_TEST is True. Using mock data.\n",
            "Using mock data for LinkedIn profile.\n",
            "\n",
            "Profile data is now available (either real or mock).\n",
            "Name: Mock User Name\n",
            "Headline: Lead AI Engineer at MockCorp | Generative AI Specialist\n",
            "Number of experiences: 2\n",
            "\n",
            "Full Profile Data Structure (first 500 chars):\n",
            "{\n",
            "  \"id\": \"mock-user-123\",\n",
            "  \"name\": \"Mock User Name\",\n",
            "  \"city\": \"Mock City, Mock Country\",\n",
            "  \"country_code\": \"XX\",\n",
            "  \"position\": \"Lead AI Engineer at MockCorp | Generative AI Specialist\",\n",
            "  \"current_company\": {\n",
            "    \"name\": \"MockCorp\",\n",
            "    \"company_id\": \"mockcorp\",\n",
            "    \"title\": \"Lead AI Engineer\",\n",
            "    \"location\": null\n",
            "  },\n",
            "  \"experience\": [\n",
            "    {\n",
            "      \"title\": \"Lead AI Engineer\",\n",
            "      \"description\": \"Developed scalable AI solutions and drove innovation in the tech industry. Specialized in deep...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Convert Profile to LlamaIndex Document\n",
        "def convert_profile_to_document(profile_data: dict) -> Document:\n",
        "    if not profile_data:\n",
        "        return Document(text=\"No profile data available.\", metadata={})\n",
        "\n",
        "    # This function is purely dependent on its input argument 'profile_data'\n",
        "\n",
        "    full_name = profile_data.get('name', 'N/A')\n",
        "    headline = profile_data.get('position', 'N/A')\n",
        "\n",
        "    experiences_text = \"\"\n",
        "    if 'experience' in profile_data and profile_data['experience']:\n",
        "        experiences_text = \"Experiences:\\n\"\n",
        "        for exp in profile_data['experience']:\n",
        "            company = exp.get('company', 'N/A')\n",
        "            title = exp.get('title', 'N/A')\n",
        "            start = exp.get('start_date', 'N/A')\n",
        "            end = exp.get('end_date', 'Present')\n",
        "            description = exp.get('description', '').replace('\\n', ' ').strip()\n",
        "            experiences_text += f\"- {title} at {company} ({start} - {end}). Description: {description}\\n\"\n",
        "\n",
        "    education_text = \"\"\n",
        "    if 'education' in profile_data and profile_data['education']:\n",
        "        education_text = \"Education:\\n\"\n",
        "        for edu in profile_data['education']:\n",
        "            school = edu.get('title', 'N/A')\n",
        "            degree = edu.get('degree', 'N/A')\n",
        "            field = edu.get('field', 'N/A')\n",
        "            education_text += f\"- {degree} in {field} from {school}\\n\"\n",
        "\n",
        "    projects_text = \"\"\n",
        "    if 'projects' in profile_data and profile_data['projects']:\n",
        "        projects_text = \"Projects:\\n\"\n",
        "        for proj in profile_data['projects']:\n",
        "            title = proj.get('title', 'N/A')\n",
        "            description = proj.get('description', '').replace('\\n', ' ').strip()\n",
        "            projects_text += f\"- Project: {title}. Description: {description}\\n\"\n",
        "\n",
        "    activity_text = \"\"\n",
        "    if 'activity' in profile_data and profile_data['activity']:\n",
        "        activity_text = \"Recent Activity (Liked Posts/Interactions):\\n\"\n",
        "        for act in profile_data['activity']:\n",
        "            interaction = act.get('interaction', 'N/A')\n",
        "            title = act.get('title', 'N/A')\n",
        "            if title:\n",
        "                activity_text += f\"- {interaction}: \\\"{title}\\\"\\n\"\n",
        "\n",
        "    document_content = (\n",
        "        f\"LinkedIn Profile: {full_name}\\n\"\n",
        "        f\"Headline: {headline}\\n\\n\"\n",
        "        f\"{experiences_text}\\n\"\n",
        "        f\"{education_text}\\n\"\n",
        "        f\"{projects_text}\\n\"\n",
        "        f\"{activity_text}\"\n",
        "    )\n",
        "\n",
        "    metadata = {\n",
        "        \"full_name\": full_name,\n",
        "        \"headline\": headline,\n",
        "        \"linkedin_url\": profile_data.get('url', 'N/A'),\n",
        "        \"source\": \"LinkedIn Profile Data Collector (Bright Data)\"\n",
        "    }\n",
        "\n",
        "    return Document(text=document_content, metadata=metadata)\n",
        "\n",
        "# The following 'if' block is for standalone testing in Colab, not used by Gradio's call to this function\n",
        "if 'profile_data' in locals() and profile_data:\n",
        "    linkedin_document = convert_profile_to_document(profile_data)\n",
        "    print(\"Converted LinkedIn profile to LlamaIndex Document.\")\n",
        "else:\n",
        "    print(\"No profile data available to convert to LlamaIndex Document. Please run Cell 5.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-vB7KPSu31o",
        "outputId": "bd638fb9-f57a-4a64-d6b1-943151a74e8d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted LinkedIn profile to LlamaIndex Document.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Node Parsing (Chunking the Document)\n",
        "# Ensure 'linkedin_document' is defined from Cell 6\n",
        "if 'linkedin_document' in locals() and linkedin_document:\n",
        "    documents_to_parse = [linkedin_document]\n",
        "\n",
        "    node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
        "    nodes = node_parser.get_nodes_from_documents(documents_to_parse)\n",
        "\n",
        "    print(f\"Parsed document into {len(nodes)} nodes (chunks).\")\n",
        "else:\n",
        "    print(\"No LinkedIn document available to parse into nodes. Please ensure Cell 6 ran successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdgW9WTnwSh0",
        "outputId": "8ada68bf-9caf-4183-f18f-596446c55f31"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed document into 1 nodes (chunks).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Embedding Model Setup\n",
        "embed_model = HuggingFaceEmbedding(\n",
        "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# IMPORTANT: ChromaDB client and storage_context initialization\n",
        "# are now moved into the Gradio function (Cell 11) and will use an IN-MEMORY client\n",
        "# to avoid 'readonly database' errors.\n",
        "\n",
        "print(\"Embedding model initialized.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QLZgFebSwTiQ",
        "outputId": "07a205d9-12ce-4379-ebb0-1f764cf48f89"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configure LLM for LlamaIndex\n",
        "# Configure the OpenAI LLM for LlamaIndex\n",
        "llama_index_openai_llm = OpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=512,\n",
        "    api_key=os.environ.get('OPENAI_API_KEY')\n",
        ")\n",
        "\n",
        "# Set the default LLM and embedding model for LlamaIndex globally\n",
        "Settings.llm = llama_index_openai_llm\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "# IMPORTANT: VectorStoreIndex creation is now handled within the Gradio function (Cell 11)\n",
        "# or for standalone testing in Cell 10.\n",
        "\n",
        "print(\"LlamaIndex LLM configured.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcr26qIywaVo",
        "outputId": "36ca0db8-a61f-4254-ca59-c3ba545ea44d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaIndex LLM configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Global Icebreaker Prompt Definition\n",
        "# The prompt is now more generic, focusing on broader professional aspects.\n",
        "global_icebreaker_prompt_string = \"\"\"\n",
        "You are an AI assistant specialized in generating personalized icebreakers for networking events.\n",
        "Your goal is to help someone start a conversation by referencing details from a person's LinkedIn profile.\n",
        "Generate 1-2 unique and engaging icebreaker questions or statements.\n",
        "\n",
        "**Focus on their:**\n",
        "- Current or past roles and responsibilities\n",
        "- Companies they've worked for\n",
        "- Overall professional journey or career highlights\n",
        "- General areas of expertise or skills\n",
        "- Educational background\n",
        "\n",
        "Avoid overly specific project details or recent social media activities unless they are highly prominent.\n",
        "Do not ask generic questions like \"What do you do?\" or \"How are you?\".\n",
        "Make it sound natural, conversational, and demonstrate you've genuinely reviewed their profile.\n",
        "\n",
        "Profile Information:\n",
        "{context_str}\n",
        "\n",
        "Generate an icebreaker for this person:\n",
        "\"\"\"\n",
        "\n",
        "global_icebreaker_prompt_template = PromptTemplate(global_icebreaker_prompt_string)\n",
        "\n",
        "print(\"Global icebreaker prompt defined (more generic).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGggxENZGIIW",
        "outputId": "d60b2075-33a2-4584-d443-837b8d666f3b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global icebreaker prompt defined (more generic).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate Icebreaker using LlamaIndex Query Engine (Standalone Test)\n",
        "\n",
        "# Removed: from llama_index.core.prompts import PromptTemplate (now in Cell 2)\n",
        "\n",
        "# --- Re-create VectorStoreIndex for standalone testing in this cell ---\n",
        "# This uses the globally defined embed_model.\n",
        "if 'nodes' in locals() and nodes:\n",
        "    # Re-initialize ChromaDB client and collection for standalone run\n",
        "    db_standalone = chromadb.Client() # Use in-memory client for standalone test\n",
        "    chroma_collection_standalone = db_standalone.get_or_create_collection(\"linkedin_icebreaker_collection_standalone\")\n",
        "    vector_store_standalone = ChromaVectorStore(chroma_collection=chroma_collection_standalone)\n",
        "    storage_context_standalone = StorageContext.from_defaults(vector_store=vector_store_standalone)\n",
        "\n",
        "    index = VectorStoreIndex(\n",
        "        nodes = nodes,\n",
        "        storage_context=storage_context_standalone, # Use standalone storage context\n",
        "        embed_model=embed_model # Use the globally initialized embed_model\n",
        "    )\n",
        "    print(\"Standalone VectorStoreIndex created for Cell 10.\")\n",
        "else:\n",
        "    print(\"Nodes not available to create standalone VectorStoreIndex. Please ensure Cell 7 ran successfully.\")\n",
        "    # Exit if nodes are not available, as the rest of the cell won't work\n",
        "    raise SystemExit(\"Cannot proceed with icebreaker generation in Cell 10 without nodes.\")\n",
        "\n",
        "\n",
        "# Use the globally defined prompt template (from Cell 9.5)\n",
        "query_engine = index.as_query_engine(\n",
        "    response_mode=\"compact\",\n",
        "    text_qa_template=global_icebreaker_prompt_template # Use the global prompt\n",
        ")\n",
        "\n",
        "# The 'query' here is not a search query, but an instruction to the LLM\n",
        "# to generate an icebreaker using the context it retrieves.\n",
        "# Adjusted for more generic focus, matching the new global prompt.\n",
        "query_for_icebreaker = \"Generate a personalized icebreaker question or statement for this person, focusing on their professional background or key achievements.\"\n",
        "\n",
        "# Get the response (the generated icebreaker)\n",
        "response = query_engine.query(query_for_icebreaker)\n",
        "\n",
        "print(\"\\n--- Generated Icebreaker ---\")\n",
        "print(response.response)\n",
        "print(\"--------------------------\")\n",
        "\n",
        "#You can also access the source nodes that were used to generate the response\n",
        "print(\"\\n--- Source Nodes Used ---\") # Commented out for cleaner output, uncomment if needed\n",
        "for node in response.source_nodes:\n",
        "    print(f\"Score: {node.score:.2f}\")\n",
        "    print(f\"Text: {node.text[:200]}...\")\n",
        "    print(\"-\" * 20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7LSF2k9wee1",
        "outputId": "5958888d-b08e-4a19-a2ac-87f805bd2bf6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standalone VectorStoreIndex created for Cell 10.\n",
            "\n",
            "--- Generated Icebreaker ---\n",
            "\"Hi Ali, I see you're a Computer Science Major with a focus on AI/ML at MIU. Your recent specialization in Meta Back-End Development caught my eye! How has that certification impacted your approach to software and machine learning development?\"\n",
            "--------------------------\n",
            "\n",
            "--- Source Nodes Used ---\n",
            "Score: 0.25\n",
            "Text: LinkedIn Profile: Mock User Name\n",
            "Headline: Lead AI Engineer at MockCorp | Generative AI Specialist\n",
            "\n",
            "Experiences:\n",
            "- Lead AI Engineer at MockCorp (Jan 2022 - Present). Description: Developed scalable AI...\n",
            "--------------------\n",
            "Score: 0.25\n",
            "Text: 😄 Last week, I opened Manus just to play around with the 2,000 credits I’d earned…\"\n",
            "- Liked by Ali Abdallah: \"ازاي تلاقي شغل ريموتلي؟ (من البيت) هقولك على أهم المواقع اللي فعلاً بتساعد ناس تلاقي شغل ر...\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Gradio Interface Function\n",
        "\n",
        "# Global variables to store the index and chat engine\n",
        "current_profile_index = None\n",
        "current_chat_engine = None\n",
        "\n",
        "def generate_icebreaker_and_setup_chat(linkedin_url: str):\n",
        "    \"\"\"\n",
        "    Generates an icebreaker and sets up the chat engine for the given URL.\n",
        "    This function will be called by the Gradio interface.\n",
        "    \"\"\"\n",
        "    global current_profile_index, current_chat_engine\n",
        "\n",
        "    brightdata_key = userdata.get('BRIGHTDATA_API_KEY')\n",
        "    if not brightdata_key:\n",
        "        # Return initial state for outputs, including hiding chat components\n",
        "        # (7 outputs: icebreaker_output, profile_index_state, chat_engine_state, chatbot_component, textbox_component, chat_submit_button, chat_clear_button)\n",
        "        return \"Error: Bright Data API key not found.\", None, None, gr.update(visible=False), gr.update(visible=False), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "    try:\n",
        "        # Use in-memory ChromaDB client for each request\n",
        "        db_gradio = chromadb.Client() # Use in-memory client\n",
        "        chroma_collection_gradio = db_gradio.get_or_create_collection(\"linkedin_icebreaker_collection\")\n",
        "\n",
        "        # This ensures no old data from previous runs of the Gradio app persists in memory.\n",
        "        try:\n",
        "            chroma_collection_gradio.delete(ids=chroma_collection_gradio.get()['ids'])\n",
        "            print(\"DEBUG: Cleared existing data in in-memory ChromaDB collection.\")\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Could not clear ChromaDB collection: {e}\")\n",
        "\n",
        "\n",
        "        vector_store_gradio = ChromaVectorStore(chroma_collection=chroma_collection_gradio)\n",
        "        storage_context_gradio = StorageContext.from_defaults(vector_store=vector_store_gradio)\n",
        "\n",
        "        gr.Info(\"Fetching LinkedIn profile data... This may take a few minutes.\")\n",
        "        profile_data_for_current_request = fetch_linkedin_profile_brightdata(\n",
        "            linkedin_url=linkedin_url,\n",
        "            brightdata_api_key=brightdata_key,\n",
        "            use_mock_data=False # Always try real data for Gradio\n",
        "        )\n",
        "\n",
        "        if not profile_data_for_current_request:\n",
        "            return \"Failed to fetch LinkedIn profile data. Please check the URL or try again later.\", None, None, gr.update(visible=False), gr.update(visible=False), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        # --- DEBUG: Confirm which profile data is being processed ---\n",
        "        print(f\"DEBUG: Profile name from fetched data: {profile_data_for_current_request.get('name', 'N/A')}\")\n",
        "\n",
        "        gr.Info(\"Profile data fetched. Converting to LlamaIndex Document...\")\n",
        "        linkedin_document = convert_profile_to_document(profile_data_for_current_request)\n",
        "\n",
        "        gr.Info(\"Document converted. Parsing into nodes...\")\n",
        "        nodes = node_parser.get_nodes_from_documents([linkedin_document])\n",
        "        if not nodes:\n",
        "            return \"Error: Could not parse profile data into readable chunks.\", None, None, gr.update(visible=False), gr.update(visible=False), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        gr.Info(f\"Parsed into {len(nodes)} nodes. Creating VectorStoreIndex...\")\n",
        "        index_gradio = VectorStoreIndex(\n",
        "            nodes=nodes,\n",
        "            storage_context=storage_context_gradio,\n",
        "            embed_model=embed_model\n",
        "        )\n",
        "        current_profile_index = index_gradio # Store globally for chat\n",
        "\n",
        "        gr.Info(\"VectorStoreIndex created. Generating icebreaker...\")\n",
        "\n",
        "        # Generate Icebreaker (using the global, more generic prompt)\n",
        "        query_engine_gradio = index_gradio.as_query_engine(\n",
        "            response_mode=\"compact\",\n",
        "            text_qa_template=global_icebreaker_prompt_template # Use the global prompt\n",
        "        )\n",
        "\n",
        "        # Adjusted for more generic focus.\n",
        "        query_for_icebreaker = \"Generate a personalized icebreaker question or statement for this person, focusing on their professional background or key achievements.\"\n",
        "\n",
        "        icebreaker_response = query_engine_gradio.query(query_for_icebreaker)\n",
        "\n",
        "        gr.Info(\"Icebreaker generated! Setting up chat...\")\n",
        "\n",
        "        current_chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
        "            query_engine=index_gradio.as_query_engine(),\n",
        "            llm=llama_index_openai_llm,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        # --- FIX: Return exactly 7 values ---\n",
        "        return icebreaker_response.response, \\\n",
        "               index_gradio, \\\n",
        "               current_chat_engine, \\\n",
        "               gr.update(visible=True), \\\n",
        "               gr.update(visible=True), \\\n",
        "               gr.update(visible=True), \\\n",
        "               gr.update(visible=True) # This is the 7th output\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: An error occurred during icebreaker generation or chat setup: {e}\")\n",
        "        # Ensure consistent return of 7 values even on error\n",
        "        return f\"An error occurred: {e}. Please check the Colab logs for details.\", \\\n",
        "               None, None, \\\n",
        "               gr.update(visible=False), \\\n",
        "               gr.update(visible=False), \\\n",
        "               gr.update(visible=False), \\\n",
        "               gr.update(visible=False) # This is the 7th output on error\n",
        "\n",
        "\n",
        "def chat_with_profile(message: str, history: list, chat_engine_state: CondenseQuestionChatEngine):\n",
        "    \"\"\"\n",
        "    Handles conversational chat with the LLM using the indexed profile.\n",
        "    \"\"\"\n",
        "    if not chat_engine_state:\n",
        "        return \"Please generate an icebreaker first by submitting a LinkedIn URL.\"\n",
        "\n",
        "    try:\n",
        "        response = chat_engine_state.chat(message)\n",
        "        # --- FIX: Append messages in the required {\"role\": \"...\", \"content\": \"...\"} format ---\n",
        "        history.append({\"role\": \"user\", \"content\": message})\n",
        "        history.append({\"role\": \"assistant\", \"content\": str(response)})\n",
        "        return history, \"\" # Return history and clear textbox\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: An error occurred during chat: {e}\")\n",
        "        # --- FIX: Append error message in the required {\"role\": \"...\", \"content\": \"...\"} format ---\n",
        "        history.append({\"role\": \"user\", \"content\": message})\n",
        "        history.append({\"role\": \"assistant\", \"content\": f\"Error: {e}\"})\n",
        "        return history, \"\" # Return history and clear textbox even on error\n",
        "\n",
        "print(\"Gradio interface function defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfF6If7-SdzU",
        "outputId": "acdf1425-2abf-4d98-ee64-67bc8aa44b80"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradio interface function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Launch Gradio Interface with Chat\n",
        "\n",
        "# Global variables to store the index and chat engine\n",
        "# These are managed by the functions in Cell 11.\n",
        "current_profile_index = None\n",
        "current_chat_engine = None\n",
        "\n",
        "# The generate_icebreaker_and_setup_chat and chat_with_profile functions\n",
        "# are defined in Cell 11 and are accessible here.\n",
        "\n",
        "with gr.Blocks(theme=\"soft\") as demo:\n",
        "    gr.Markdown(\"# LinkedIn Icebreaker & Chat Bot\")\n",
        "    gr.Markdown(\"Enter a public LinkedIn profile URL to generate a personalized icebreaker. Once generated, you can chat with the model about the profile!\")\n",
        "\n",
        "    profile_index_state = gr.State(None)\n",
        "    chat_engine_state = gr.State(None)\n",
        "\n",
        "    with gr.Row():\n",
        "        linkedin_url_input = gr.Textbox(label=\"LinkedIn Profile URL\", placeholder=\"e.g., https://www.linkedin.com/in/example-user-profile/\", scale=2)\n",
        "        submit_button = gr.Button(\"Generate Icebreaker\", scale=1)\n",
        "\n",
        "    icebreaker_output = gr.Textbox(label=\"Generated Icebreaker\", interactive=False, lines=3)\n",
        "\n",
        "    with gr.Column(visible=False) as chat_column: # Group chat components in a column, initially hidden\n",
        "        chatbot_component = gr.Chatbot(height=300, type='messages') # Added type='messages' to suppress warning\n",
        "        textbox_component = gr.Textbox(placeholder=\"Ask a follow-up question about the profile...\", container=False, scale=7)\n",
        "        with gr.Row(): # Group chat buttons in a row\n",
        "            chat_submit_button = gr.Button(\"Ask\")\n",
        "            chat_clear_button = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    # Link the chat components to a ChatInterface-like behavior\n",
        "    textbox_component.submit(\n",
        "        fn=chat_with_profile,\n",
        "        inputs=[textbox_component, chatbot_component, chat_engine_state],\n",
        "        outputs=[chatbot_component, textbox_component], # Clear textbox after submit\n",
        "        queue=False\n",
        "    )\n",
        "\n",
        "    chat_submit_button.click(\n",
        "        fn=chat_with_profile,\n",
        "        inputs=[textbox_component, chatbot_component, chat_engine_state],\n",
        "        outputs=[chatbot_component, textbox_component],\n",
        "        queue=False\n",
        "    )\n",
        "\n",
        "    chat_clear_button.click(\n",
        "        lambda: [], # Returns empty list to clear chatbot\n",
        "        outputs=[chatbot_component],\n",
        "        queue=False\n",
        "    )\n",
        "\n",
        "    submit_button.click(\n",
        "        fn=generate_icebreaker_and_setup_chat,\n",
        "        inputs=[linkedin_url_input],\n",
        "        # --- FIX: Corrected outputs list to match the 7 values returned by the function ---\n",
        "        outputs=[icebreaker_output, profile_index_state, chat_engine_state,\n",
        "                 chatbot_component, textbox_component, chat_submit_button, chat_clear_button]\n",
        "    ).success(\n",
        "        # On success, update the visibility of the entire chat column AND clear the chatbot\n",
        "        lambda: [gr.update(visible=True), []], # Return update for column visibility and empty list for chatbot\n",
        "        outputs=[chat_column, chatbot_component]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "TC4KfgbESerE",
        "outputId": "cedbd0ab-8c02-41f1-c7b0-5189aa0b5809"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://25c301449ae098ab9d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://25c301449ae098ab9d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Cleared existing data in in-memory ChromaDB collection.\n",
            "Fetching profile for: https://www.linkedin.com/in/sarah-helal/\n",
            "Collection triggered. Snapshot ID: s_mdaymfq629makqus9x. Polling for status...\n",
            "Polling... (1/60)\n",
            "✅ Snapshot s_mdaymfq629makqus9x is ready!\n",
            "Retrieving data from Bright Data...\n",
            "✅ Data retrieved successfully!\n",
            "DEBUG: Profile name from fetched data: Sarah Helal\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://25c301449ae098ab9d.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ]
}